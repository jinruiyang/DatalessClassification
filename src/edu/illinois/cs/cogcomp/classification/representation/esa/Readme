1. SimpleESA. This is Vivek's approach, which is pretty good but slower for online test. There is no explicit IDF computation. It just use the whole document to search the index, and retrieve relevant Wikipedia documents.

2. ComplexESAVivek. This approach uses Vivek's method to search each word. For example, w_i -> C_i={c_{i1}, c_{i2} ...}. Then when combining them, the document concept set is generated by \sum_i c_ij * TFIDF_i. The TFIDF is computed as follow:

It first computes the tf_i of word w_i in the document. Then it gets TFIDF_i = TF_i*IDF_i (where IDF_i is based on Wikipedia). Then normalizes TFIDF_i using L-2 norm for the document.

3. ComplexESA. This approach is based on the implementation here (which is linked by Evgeniy Gabrilovich http://www.cs.technion.ac.il/~gabr/resources/code/esa/esa.html, https://github.com/faraday/wikiprep-esa/blob/master/README).

I modify it to be more similar to the approach shown in http://www.jair.org/media/2669/live-2669-4346-jair.pdf.

This approach is with following steps.
 
    Indexing: Build inverted index of words.

        Step 1: Build index of documents.

        Step 2: For each word, it computes a score for the pair of word and the concept (title of Wikipedia page). I extended this with five scores:

        1. tf: Just use the TF score in the Wikipedia page for each word. 
        
        2. tfidf: Use the TFIDF (IDF is pre-computed by Lucene) for each word.

        3. tfidfnorm: After have TFIDF of all words, I normalize them in the document (which is just the Wiki page) to let the document has uniform norm (L2 norm).

        4. tfidfboost: Multiply the page inlink number to each TFIDF score, which is the same for all  .

        5. tfidfboostnorm: After normalization of TFIDF score to have the 1 L2-norm for each document, it multiplies the page inlink number.


    ESA retrieval: For each document, generate the word scores:

        1 Count each word's TF.

        2 Two ways to compute IDF:

        Wiki-based: retrieve idf from Wikipedia indexing. (the results reported are based on this)

        Corpus-base: compute IDF based on the corpus.

        3 Normalize this vector of 1/TF/TFIDF to have the L2-norm to be 1.

        4 To generate a normalize vector of each document, combine the word concept vectors with three ways:

        \sum_i c_ij * 1 (which I called DF)

        \sum_i c_ij * TF_i

        \sum_i c_ij * TFIDF_i ( the results reported are based on this)

        5 Sort the scores and retain the top 500 concepts.

